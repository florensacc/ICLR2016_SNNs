\documentclass{article} % For LaTeX2e
\usepackage{iclr2017_conference,times}
\usepackage{hyperref}
\usepackage{url}
% \usepackage[dvips]{graphicx}

% from Exploring Policy Rep
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[normalem]{ulem}
\usepackage{dsfont}
\usepackage{enumitem}

% For algorithms
\usepackage[algo2e, ruled]{algorithm2e}

\usepackage[center]{subfigure}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\EE}{\mathbb{E}}
\usepackage{changepage}
\usepackage{graphicx, wrapfig}

\usepackage{pdfpages}

\usepackage{cancel}

\title{Stochastic Neural Networks for \\Hierarchical Reinforcement Learning}


\author{Carlos Florensa${^\dagger}$, Yan Duan${^\dagger}{^\ddagger}$, Pieter Abbeel${^\dagger}{^\ddagger}{^\mathsection}$ \\
$^\dagger$ UC Berkeley, Department of Electrical Engineering and Computer Science\\
$^\mathsection$ International Computer Science Institute\\
$^\ddagger$ OpenAI\\
% Department of Electrical Engineering and Computer Sciences\\
% UC Berkeley\\
\texttt{florensa@berkeley.edu, \{rocky,pieter\}@openai.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\newcommand{\sset}{\mathcal{S}}
\newcommand{\aset}{\mathcal{A}}
\newcommand{\oset}{\mathcal{O}}
\newcommand{\mdpset}{\mathcal{M}}
\newcommand{\trans}{\mathcal{P}}
\newcommand{\agent}{\mathrm{agent}}
\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}
	
	\maketitle

\begin{abstract}


%%%%





%%%%

Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these challenges, there are two common approaches: The first approach is to design a hierarchy over the action space, which tends to require domain-specific knowledge and careful hand-engineering. The second approach utilizes domain-agnostic intrinsic rewards to guide exploration, which has been shown to be effective in tasks with sparse rewards. However, it is unclear how knowledge acquired for solving one task can be utilized for other tasks, leading to a high sample complexity overall when considering a collection of tasks. In this paper, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the learned skills for downstream tasks by training a high-level policy over these skills. As such, our approach is bringing together some of the strengths of exploration methods (at pre-training) and hierarchical methods (when learning the downstream tasks).  To pre-train skills, we use stochastic neural networks (SNNs) combined with a proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and, when used on downstream tasks, can significantly boost the learning performance uniformly across a wide range of tasks. 


\end{abstract}

\section{Introduction}

In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixel inputs \citep{guo2014deep, mnih2015human, Schulman15TRPO}, mastering the game of Go \citep{silver2016mastering}, and acquiring advanced manipulation and locomotion skills from raw sensory inputs \citep{Schulman15TRPO, lillicrap2015continuous, watter2015embed, schulman2016high, heess2015learning, levine2016end}.
% These successes can be traced back to much earlier work \citep{Tesauro95TDGammon, Bertsekas95Neuro}.
Despite these success stories, these deep RL algorithms typically employ naive exploration strategies such as $\epsilon$-greedy or uniform Gaussian exploration noise, which have been shown to perform poorly in tasks with sparse rewards \citep{duan2016benchmarking, houthooft2016variational, bellemare2016unifying}.
Tasks with sparse rewards are common in realistic scenarios. 
For example, in navigation tasks, the agent is not rewarded until it finds the target. The challenge is further complicated by long horizons, where naive exploration strategies can lead to exponentially large sample complexity \citep{osband2014generalization}.

To tackle these challenges, two main strategies have been pursued:
The first strategy is to design a hierarchy over the actions \citep{parr1998reinforcement, sutton1999between, dietterich2000hierarchical}.
By composing low-level actions into high-level primitives, the search space can be reduced exponentially. However, these approaches require domain-specific knowledge and careful hand-engineering. The second strategy uses intrinsic rewards to guide exploration \citep{schmidhuber1991curious, schmidhuber2010formal, houthooft2016variational, bellemare2016unifying}.
The computation of these intrinsic rewards does not require domain-specific knowledge.
However, when facing a collection of tasks, these methods do not provide a direct answer about how knowledge about solving one task may transfer to other tasks, and by solving each of the tasks from scratch, the overall sample complexity may still be high.

In this paper, we propose a general framework for training policies for a collection of tasks with sparse rewards.
Our framework first learns a span of skills in a pre-training environment, where it is employed with nothing but a proxy reward signal, whose design only requires very minimal domain knowledge about the downstream tasks. This proxy reward can be understood as a form of intrinsic motivation that encourages the agent to explore its own capabilities, without any goal information or sensor readings specific to each downstream task.
The set of skills can be used later in a wide collection of different tasks, by training a separate high-level policy for each task on top of the skills, thus reducing sample complexity uniformly.
To learn the span of skills, we propose to use stochastic neural networks (SNNs) \citep{neal1990learning, neal1992connectionist, Tang2014_FSNN}, a class of neural networks with stochastic units in the computation graph.
This class of architectures can easily represent multi-modal policies, while achieving weight sharing among the different modes.
We parametrize the stochasticity in the network by feeding latent variables with simple distributions as extra input to the policy.
In the experiments, we observed that direct application of SNNs does not always guarantee that a wide range of skills will be learned. 
Hence, we propose an information-theoretic regularizer based on Mutual Information (MI) in the pre-training phase to encourage diversity of behaviors of the SNN policy.
Our experiments find that our hierarchical policy-learning framework can learn a wide range of skills, which are clearly interpretable as the latent code is varied. Furthermore, we show that training high-level policies on top of the learned skills can results in strong performance on a set of challenging tasks with long horizons and sparse rewards.

\section{Related Work}

One of the main appealing aspects of hierarchical reinforcement learning (HRL) is to use skills to reduce the search complexity of the problem \citep{parr1998reinforcement, sutton1999between, dietterich2000hierarchical}.
However, specifying a good hierarchy by hand requires domain-specific knowledge and careful engineering, hence motivating the need for learning skills automatically. Prior work on automatic learning of skills has largely focused on learning skills in discrete domains \citep{chentanez2004intrinsically, vigorito2010intrinsically}.
A popular approach there is to use statistics about state transitions to identify bottleneck states \citep{stolle2002learning, mannor2004dynamic, csimcsek2005identifying}.
It is however not clear how these techniques can be applied to settings with high-dimensional continuous spaces.
More recently, \cite{mnih2016strategic} propose a DRAW-like \citep{gregor2015draw} recurrent neural network architecture that can learn temporally extended macro actions.
However, the learning needs to be guided by external rewards as supervisory signals, and hence the algorithm cannot be straightforwardly applied in sparse reward settings.

There has also been work on learning skills in tasks with continuous actions \citep{schaal2005learning, konidaris2011autonomous, daniel2013autonomous, ranchod2015nonparametric}.
These methods extract useful skills from successful trajectories for the same (or closely related) tasks, and hence require first solving a comparably challenging task or demonstrations. 
%Guided Policy Search (GPS)~\cite{levine2016end} leverages access to training in a simpler setting.  GPS trains with iLQG~\cite{todorov2005ilqg} in state space, and in parallel trains a neural net that only receives raw sensory inputs that has to agree with the iLQG controller.  The neural net policy is then able to generalize to new situations.


Another line of work on skill discovery particularly relevant to our approach is the HiREPS algorithm by \cite{daniel2012hierarchical} where, instead of having a mutual information bonus like in our proposed approach, they introduce a constraint on an equivalent metric. The solution approach is nevertheless very different as they cannot use policy gradients. Furthermore, although they do achieve multimodality like us, they only tried the episodic case where a single option is active during the rollout. Hence the hierarchical use of the learned skills is less clear. Similarly, the Option-critic architecture \citep{bacon2016option} can learn interpretable skills, but whether they can be reuse across complex tasks is still an open question. Recently, \cite{heess2016learning} have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework.
However, their pre-training setup requires a set of goals to be specified.
In comparison, we use proxy rewards as the only signal to the agent during the pre-training phase, the construction of which only requires minimal domain knowledge or instrumentation of the environment.


\section{Preliminaries}
    
    We define a discrete-time finite-horizon discounted Markov decision process (MDP) by a tuple $M = (\sset, \aset, \trans, r, \rho_0, \gamma, T)$, in which $\sset$ is a state set, $\aset$ an action set, $\trans: \sset \times \aset \times \sset \rightarrow \mathbb{R}_{+}$ a transition probability distribution, $r: \sset \times \aset \rightarrow [-R_{\max}, R_{\max}]$ a bounded reward function, $\rho_0: \sset \to \mathbb{R}_+$ an initial state distribution, $\gamma \in [0, 1]$ a discount factor, and $T$ the horizon. When necessary, we attach a suffix to the symbols to resolve ambiguity, such as $\sset^M$. In policy search methods, we typically optimize a stochastic policy $\pi_{\theta}: \sset \times \aset \to \mathbb{R}_+$ parametrized by $\theta$. The objective is to maximize its expected discounted return, $ \eta(\pi_\theta) = \mathbb{E}_{\tau}[ \sum_{t=0}^T \gamma^t r(s_t, a_t) ]$, where $\tau = (s_0, a_0, \ldots)$ denotes the whole trajectory, $\displaystyle s_0 \sim \rho_0(s_0)$, $a_t \sim \pi_\theta(a_t|s_t)$, and $s_{t+1} \sim \trans(s_{t+1} | s_t, a_t)$.

\section{Problem statement}

Our main interest is in solving a collection of downstream tasks, specified via a collection of MDPs $\mdpset$. If these tasks do not share any common structure, we cannot expect to acquire a set of skills that will speed up learning for all of them. On the other hand, we want the structural assumptions to be minimal, to make our problem statement more generally applicable.

For each MDP $M \in \mdpset$, we assume that the state space $\sset^M$ can be factored into two components, $\sset_\agent$, and $\sset_{\mathrm{rest}}^M$, which only weakly interact with each other.
The $\sset_\agent$ should be the same for all MDPs in $\mdpset$. We also assume that all the MDPs share the same action space.
Intuitively, we consider a robot who faces
a collection of tasks, where the dynamics of the robot are shared across tasks, and are covered by $\sset_\agent$, but there may be other components in a task-specific state space, which will be denoted by $\sset_{\mathrm{rest}}$. For instance, in a grasping task $M$, $\sset_{\mathrm{rest}}^M$ may include the positions of objects at interest or the new sensory input associated with the task. This specific structural assumption has been studied in the past as sharing the same \emph{agent-space} \citep{konidaris2007building}.

Given a collection of tasks satisfying the structural assumption, our objective for designing the algorithm is to minimize the total sample complexity required to solve these tasks. This has been more commonly studied in the past in sequential settings, where the agent is exposed to a sequence of tasks, and should learn to make use of experience gathered from solving earlier tasks to help solve later tasks \citep{taylor2009transfer, wilson2007multi, lazaric2010bayesian, devin2016learning}. However, this requires that the earlier tasks are relatively easy to solve (e.g., through good reward shaping or simply being easier) and is not directly applicable when all the tasks have sparse rewards, which is a much more challenging setting. In the next section, we describe our formulation, which takes advantage of a pre-training task that can be constructed with minimal domain knowledge, and can be applied to the more challenging scenario.

\section{Methodology}
\label{sec:method}

In this section, we describe our formulation to solve a collection of tasks, exploiting the structural assumption articulated above. In Sec.~\ref{section:method:pretraining}, we describe the pre-training environment, where we use proxy rewards that will be useful for learning a span of skills. In Sec.~\ref{section:method:snn}, we motivate the usage of stochastic neural networks (SNNs) and discuss the architectural design choices made to tailor them to skill learning for RL. 
In Sec.~\ref{section:method:inforeg}, we describe an information-theoretic regularizer that further improves the span of skills learned by SNNs. In Sec.~\ref{section:method:highlevel}, we describe the architecture of high-level policies over the learned skills, and the training procedure for the downstream tasks with sparse rewards. Finally, in Sec.~\ref{section:method:polopt}, we describe the policy optimization for both phases of training.

\subsection{Constructing the pre-training environment}
\label{section:method:pretraining}

Given a collection of tasks, we would like to construct a pre-training environment, where the agent can learn a span of skills that will be useful for downstream tasks. We achieve this by letting the agent freely interact with the environment in a minimal setup.
For a mobile robot, this can be a spacious environment where the robot can first learn the necessary locomotion skills; for a manipulator arm which will be used for object manipulation tasks, this can be an environment with many objects that the robot can interact with.

Rather than specifying goals in the pre-training environment corresponding to the desired skills, which requires precise specification about what the skills should entail, we use a proxy reward as the only reward signal to guide skill learning.
The design of the proxy reward should encourage the existence of locally optimal solutions, which will correspond to different skills the agent should learn.
For a mobile robot, this reward can be as simple as proportional to the magnitude of the speed of the robot, without constraining the direction of movement.
For a manipulator arm, this reward can be the successful grasping of any object.

\subsection{Stochastic Neural Networks for Skill Learning}
\label{section:method:snn}

Having constructed the pre-training environment, a straightforward way to learn a span of skills is to train different policies, each with a uni-modal action distribution (e.g. a multivariate Gaussian) under different random initializations, and hope that they will converge to different local optima, hence learning different skills. However, the sample complexity of this approach is at least directly proportional to the number of skills we would like to learn. This can become quite expensive when the agent learns from high-dimensional sensory signals like images, and needs to rebuild a good feature representation from scratch. In addition, we do not have any control over whether the different policies actually learn different skills. We address these two issues separately in the current and the next subsection.

To tackle the first issue, we propose to use stochastic neural networks (SNNs), a general class of neural networks with stochastic units in the computation graph. There has been a large body of prior work on special classes of SNNs, such as Restricted Boltzmann Machines (RBMs) \citep{smolensky1986information, hinton2002training}, Deep Belief Networks (DBNs) \citep{hinton2006fast}, and Sigmoid Belief Networks (SBNs) \citep{neal1990learning, Tang2014_FSNN}. They have rich representation power and can in fact approximate any well-behaved probability distributions \citep{le2008representational, cho2013gaussian}. Policies modeled via SNNs can hence represent complex action distributions, especially multi-modal distributions.

For our purpose, we use a simple class of SNNs, where latent variables with fixed distributions are integrated with inputs to the neural network (here, the observations from the environment) to form a joint embedding, which is then fed to a standard feed-forward neural network with deterministic units, that computes distribution parameters for a uni-modal distribution (e.g. the mean and variance parameters of a multivariate Gaussian). We use simple categorical distributions with uniform weights for the latent variables, where the number of classes, $K$, is a hyperparameter that upper bounds the number of skills that we would like to learn.
\begin{figure}[th]
	\centering
		\vspace{-10pt}
	\subfigure[Concatenation]{
		\centering
		\label{fig:snn_architecture_concatenate}
		\includegraphics[width = 0.3\textwidth]{Figures/SNN-concat.png}
	}
	\subfigure[Bilinear integration]{
		\centering
		\label{fig:snn_architecture_bilinear}
		\includegraphics[width = 0.5\textwidth]{Figures/bilinear-architecture-snn.png}
	}
	\vspace{-10pt}
	\caption{Different architectures for the integration of the latent variables in a FNN}
	\label{fig:snn_architecture}
		\vspace{-10pt}
\end{figure}

The simplest joint embedding, as shown in Figure~\ref{fig:snn_architecture_concatenate}, is to concatenate the observations and the latent variables directly.\footnote{In scenarios where the observation is very high-dimensional such as images, we can form a low-dimensional embedding of the observation alone, say using a neural network, before jointly embedding it with the latent variable.} However, this limits the expressiveness power of integration between the observation and latent variable. Richer forms of integrations, such as multiplicative integrations and bilinear pooling, have been shown to have greater representation power and improve the optimization landscape, achieving better results when complex interactions are needed \citep{fukui2016multimodal, wu2016multiplicative}. Inspired by this work, we study using a simple bilinear integration, by forming the outer product between the observation and the latent variable (Fig.~\ref{fig:snn_architecture_bilinear}). Note that the concatenation integration effectively corresponds to changing the bias term of the first hidden layer depending on the latent code $h$ sampled, and the bilinear integration to changing all the first hidden layer weights. As shown in the experiments, the choice of integration greatly affects the quality of the span of skills that is learned. Our bilinear integration already yields a large span of skills, hence no other type of SNNs is studied in this work.


Compared to training separate policies, training a single SNN allows for flexible weight-sharing schemes among different policies. 
To obtain temporally extended and consistent behavior associated with each latent code, in the pre-training environment we sample a latent code at the beginning of every rollout and keep it constant throughout the entire rollout.
Finally, we can easily further encourage the diversity of skills learned by the SNN by introducing an information theoretic regularizer, as detailed in the next section. 
After training, each of the latent code in the categorical distribution will correspond to a different, interpretable skill, which can then be used for the downstream tasks.


\subsection{Information-Theoretic Regularization}
\label{section:method:inforeg}

Although SNNs have sufficient expressiveness to represent multi-modal policies, there is nothing in the optimization that prevents them from collapsing into a single mode. We have not observed this worst case scenario in our experiments, but sometimes different latent codes correspond to very similar skills. It is desirable to have direct control over the diversity of skills that will be learned. To achieve this, we introduce an information-theoretic regularizer, inspired by recent success of similar objectives in encouraging interpretable representation learning in InfoGAN \cite{chen2016infogan}.

Concretely, we add an additional reward bonus, proportional to the mutual information (MI) between the latent variable and the current state. We only measure the MI with respect to a relevant subset of the state.
For a mobile robot, we choose this to be the $(x,y)$ coordinates of its center of mass (CoM). 
Formally, let $X$ be a random variable denoting the current CoM coordinate of the agent, and let $Z$ be the latent variable. Our additional reward bonus is $\alpha_H I(Z;X) = \alpha_H (H(Z) - H(Z|X))$, where $H$ denotes the entropy function. In our case, $H(Z)$ is constant since the probability distribution of the latent variable is fixed to uniform during this stage of training. Hence, maximizing MI is equivalent to minimizing the conditional entropy $H(Z|X)$. Therefore, another interpretation of this bonus is that given where the robot is, it should be easy to infer  which skill the robot is currently performing. 

To estimate the MI we need to estimate the posterior probability of the latent code given the state. This can be done by fitting a MLP regressor by Maximum Likelihood, but we found as effective and more efficient to apply the following discretization: we partition the $(x,y)$ coordinate space into cells $c\in\{0,1,\dots,C\}$, and map the continuous-valued CoM coordinate into the cell containing it. This way, calculation of empirical MI only requires maintaining visitation counts $n_c(z)$ of each latent code $z$ in each cell $c$, yielding:
\begin{align}
    \hat{p}(Z=z|X)\approx\hat{p}(Z=z|c)=\frac{n_c(z)}{\sum_{z'}n_c(z')}
\end{align}
Given that we use a batch policy optimization method, we use all trajectories of the current batch to compute the $n_c(z)$. Then, for every point (with CoM in cell $c_t$) in every trajectory with latent $z$, we modify the reward by adding:
\begin{align}
    \alpha_H~ \log\hat{p}(Z=z|c_t)
\end{align}


\subsection{Learning High-level Policies}
\label{section:method:highlevel}

\begin{wrapfigure}{r}{5cm}
\vspace{-40pt}
\includegraphics[width = 5cm]{Figures/hierarchical-SNN.png}
\caption{Hierarchical SNN architecture to solve downstream tasks}
\label{fig:hierarchical_snn_architecture}
\end{wrapfigure}

Given a span of skills learned from the pre-training task, we now describe how to use them as basic building blocks for solving tasks where only sparse reward signals are provided. Assuming that the categorical latent variable of the pretrained SNN has $K$ classes, and hence there are $K$ different skills, we train a high-level policy (Manager Neural Network) that operates by selecting among these different skills. 
For a given task $M \in \mdpset$, and given a known factored representation of the state space $\sset^M$ as $\sset_\agent$ and $\sset_{\mathrm{rest}}^M$, the high-level policy receives the full state as input, and outputs the parametrization of a categorical distribution from which we sample a discrete action out of $K$ possible choices, as depicted in Fig.\ \ref{fig:hierarchical_snn_architecture}. 
The high-level policy operates at a slower time scale than the SNN policy, only switching its categorical output every $\mathcal{T}$ time-steps. We call $\mathcal{T}$ the switch time and it is a hyperparameter to be determined based on the downstream task (see Appendix \ref{sec:switch_time} for a sensitivity analysis). 
The same structure could also be used to switch between $K$ full policies independently pre-trained; this will be one of our baselines (called Multi-policy) further detailed in the experiment section.

We freeze the weights of the SNN during this phase of training. In principle, they can also be jointly optimized to adapt the skills to the task at hand. This end-to-end training of a policy with discrete latent variables in the Stochastic Computation Graph could be done using straight-through estimators like the one proposed by \cite{jang2016gumbel} or \cite{maddison2016concrete}. Nevertheless, we show in our experiments that frozen low-level policies are already sufficient to achieve good performance in the studied downstream tasks, so these directions are left as future research.


\subsection{Policy Optimization}
\label{section:method:polopt}

For both the pre-training phase and the training of the high-level policies, we use Trust Region Policy Optimization (TRPO) as the policy optimization algorithm \citep{Schulman15TRPO}. We choose TRPO due to its excellent empirical performance and because it does not require excessive hyperparameter tuning. For the pre-training phase, the training of policies is complicated by the fact that due to the presence of latent variables, the marginal distribution of $\pi(a|s)$ is now a mixture of Gaussians instead of a simple Gaussian, and it may even become intractable to compute if we use more expressive latent variables. This also complicates the computation of KL divergence, which is needed to control the step size of the policy update. In our implementation, we hold the latent variable $z$ fixed in the optimization phase, and compute $\pi(a|s,z)$, without marginalizing over $z$. The rest of the TRPO procedure can then be straightforwardly applied.

\begin{algorithm2e}[H]
\SetAlgoLined
 \SetKwInOut{Input}{Input}
 \SetKwInOut{Output}{Output}
 \textbf{Initialize: }{Policy $\pi_{\theta}$; Latent dimension $K$}\;
 \While{Not trained}{
     \For{$n \leftarrow \ 1$ \KwTo $N$}{
        Sample $z_n\sim \text{Cat}\big(\frac{1}{K}\big)$\;
        Collect rollout with $z_n$ fixed\;
     }
     TRPO pdate $\theta$ with $\nabla_{\theta}\eta\approx\sum_n\sum_t\nabla_\theta\log\pi_\theta(a_t| s_t, z_t) \sum_{t'=t}\big(R_{t'}+\log\hat{p}(Z=z_n|s_t)\big)$
 }
 \caption{Skill training for SNNs}
 \label{alg:overall}
\end{algorithm2e}


\section{Experiments}
\label{sec:experiments}
We have applied our framework to the two hierarchical tasks described in the benchmark by \cite{duan2016benchmarking}: Locomotion + Maze and Locomotion + Food Collection (Gather). The observation space of these tasks naturally decompose into $S_{agent}$ being the robot and $S_{rest}^M$ the task-specific attributes like walls, goals, and sensor readings. Here we report the results using the Swimmer robot, also described in the benchmark paper. In fact, the swimmer locomotion task described therein corresponds exactly to our pretrain task, as we also solely reward speed in a plain environment. We report results with more complex robots in Appendix \ref{sec:snake}-\ref{sec:ant}.

To increase the variety of downstream tasks, we have constructed four different mazes. Maze 0 is the same as the one described in the benchmark \citep{duan2016benchmarking} and Maze 1 is its reflection, where the robot has to go backwards-right-right instead of forward-left-left. These correspond to  Figs.\ \ref{fig:Maze0}-\ref{fig:Maze1}, where the robot is shown in its starting position and has to reach the other end of the U turn. Mazes 2 and 3 are different instantiations of the environment shown in Fig.\ \ref{fig:Maze2}, where the goal has been placed in the North-East or in the South-West corner respectively. A reward of 1 is only granted when the robot reaches the goal position. In the Gather task depicted in Fig.\ \ref{fig:FoodGather}, the robot gets a reward of 1 for collecting green balls and a reward of -1 for the red ones, all of which are positioned randomly at the beginning of each episode. Apart from the robot joints positions and velocities, in these tasks the agent also receives LIDAR-like sensor readings of the distance to walls, goals or balls that are within a certain range. 

In the benchmark of continuous control problems \citep{duan2016benchmarking} it was shown that algorithms that employ naive exploration strategies could not solve them. More advanced intrinsically motivated explorations \citep{houthooft2016variational} do achieve some progress, and we report our stronger results with the exact same setting in Appendix \ref{sec:gather_compare}. Our hyperparameters for the neural network architectures and algorithms are detailed in the Appendix \ref{sec:hyper} and the full code is available\footnote{https://github.com/florensacc/snn4hrl}.

\begin{figure}[ht]
	\centering
	\subfigure[Maze 0]{
		\centering
		\label{fig:Maze0}
		\includegraphics[width = 0.2\textwidth, height=0.2\textwidth]{Figures/Maze0.png}
	}
	\subfigure[Maze 1]{
		\centering
		\label{fig:Maze1}
		\includegraphics[width = 0.2\textwidth, height=0.2\textwidth]{Figures/Maze8.png}
	}
	\subfigure[Maze 2 or 3]{
		\centering
		\label{fig:Maze2}
		\includegraphics[width = 0.2\textwidth, height=0.2\textwidth]{Figures/Maze4.png}
	}
	\subfigure[Food Gather]{
		\centering
		\label{fig:FoodGather}
		\includegraphics[width = 0.2\textwidth, height=0.2\textwidth]{Figures/FoodGatherSwimmer.png}
	}
	\vspace{-10pt}
	\caption{Illustration of the sparse reward tasks}
	\label{fig:snn_multimodal_MI}
\end{figure}


\section{Results}
\label{sec:results}
We evaluate every step of the skill learning process, showing the relevance of the different pieces of our architecture and how they impact the exploration achieved when using them in a hierarchical fashion. Then we report the results on the sparse environments described above. We seek to answer the following questions:
\begin{itemize}
\vspace{-8pt}
    \setlength\itemsep{0em}
    \item Can the multimodality of SNNs and the MI bonus consistently yield a large span of skills? 
    \item Can the pre-training experience improve the exploration in downstream environments?
    \item Does the enhanced exploration help to efficiently solve sparse complex tasks?
\end{itemize}
Appart from the explanations below, videos of the achieved results are available\footnote{\label{foot:video} \url{https://goo.gl/5wp4VP}}. 

\subsection{Skill learning in pretrain}

To evaluate the diversity of the learned skills we use ``visitation plots'', showing the $(x,y)$ position of the robot's Center of Mass (CoM) during 100 rollouts of 500 time-steps each. At the beginning of every rollout, we reset the robot to the origin, always in the same orientation (as done during training). In Fig.\ \ref{fig:visit-trpo-individuals} we show the visitation plot of six different feed-forward policies, each trained from scratch in our pre-training environment. For better graphical interpretation and comparison with the next plots of the SNN policies, Fig.\ \ref{fig:visit_trpo6_likeSNN} superposes a batch of 50 rollout for each of the 6 policies, each with a different color. Given the morphology of the swimmer, it has a natural preference for forward and backward motion. Therefore, when no extra incentive is added, the visitation concentrates heavily on the direction it is always initialized with. Note nevertheless that the proxy reward is general enough so that each independently trained policy yields a different way
of advancing, hence granting a potentially useful skills to solve downstream tasks when embedded in our described Multi-policy hierarchical architecture.

\begin{figure}[h!]
    \vspace{-10pt}
	\centering
	\subfigure[Independently trained policies in the pre-train MDP with the proxy reward of the CoM speed norm]{
		\centering
		\label{fig:visit-trpo-individuals}
		\includegraphics[trim={2cm 0 2cm 1.5cm}, clip, width=0.1\textwidth]{Figures/visit-trpo1.png}
	    \includegraphics[trim={2cm 0 2cm 1.5cm}, clip, width=0.1\textwidth]{Figures/visit-trpo2.png}
		\includegraphics[trim={2cm 0 2cm 1.5cm}, clip, width=0.1\textwidth]{Figures/visit-trpo3.png}
	    \includegraphics[trim={2cm 0 2cm 1.5cm}, clip, width=0.1\textwidth]{Figures/visit-trpo4.png}
		\includegraphics[trim={2cm 0 2cm 1.5cm}, clip, width=0.1\textwidth]{Figures/visit-trpo5.png}
	    \includegraphics[trim={2cm 0 2cm 1.5cm}, clip, width=0.1\textwidth]{Figures/visit-trpo6.png}
	}
	\subfigure[Superposed policy visitations from (a)]{
		\centering
		\label{fig:visit_trpo6_likeSNN}
	    \includegraphics[trim={2cm 0 3.6cm 1.5cm}, clip, width=0.2\textwidth]{Figures/likeSNN-hierarchized-multi-Maze-1-egoSwimmer6-pre500-1pl-500in-100Bs-1000-trpo-swimmer-ego-500_visitation.png}
	}
	\subfigure[SNN \emph{without} bilinear integration and increasing $\alpha_H=0,0.001,0.01,0.1$]{
		\centering
		\label{fig:visit_snn6_noBilinear}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-NoBil-0MI.png}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-NoBil-0001MI.png}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-NoBil-001MI.png}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-NoBil-01MI.png}
	}
	\subfigure[SNN \emph{with} bilinear integration and increasing $\alpha_H=0,0.001,0.01,0.1$]{
		\centering
		\label{fig:visit_snn6_bilinear}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-Bil-0001MI.png}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-Bil-001MI.png}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-Bil-01MI.png}
		\includegraphics[trim={2cm 0 1cm 1.5cm}, clip, width = 0.2\textwidth]{Figures/visit-snn-Bil-1MI.png}
	}
	\vspace{-10pt}
	\caption{Span of skills learn by different methods and architectures}
	\label{fig:visit_methods}
\end{figure}

Next we show that, using SNNs, we can learn a similar or even larger span of skills without training several independent policies. The number of samples used to train a SNN is the same as a single feed-forward policy from Fig.\ \ref{fig:visit-trpo-individuals}, therefore this method of learning skills effectively reduces the sample complexity by a factor equal to the numbers of skills being learned. With an adequate architecture and MI bonus, we show that the span of skills generated is also richer.   
In the two rows of Figs.\ \ref{fig:visit_snn6_noBilinear}-\ref{fig:visit_snn6_bilinear} we present the visitation plots of SNNs policies obtained for different design choices. Now the colors indicate the latent code that was sampled at the beginning of the rollout. We observe that each latent code generates a particular, interpretable behavior. Given that the initialization orientation is always the same, the different skills are truly distinct ways of moving: forward, backwards, or sideways. In the following we analyze the impact on the span of skills of the integration of latent variables (concatenation in first row and bilinear in the second) and the MI bonus (increasing coeficient $\alpha_H$ towards the right). Simple concatenation of latent variables with the observations rarely yields distinctive behaviors for each latent code sampled. On the other hand, we have observed that 80\% of the trained SNNs with bilinear integration acquire at least forward and backward motion associated with different latent codes. This can be further improved to 100\% by increasing $\alpha_H$ and we also observe that the MI bonus yields less overlapped skills and new advancing/turning skills, independently of the integration of the latent variables.

% put learning curves of the pre-train task in the appendix!!

\subsection{Hierarchical use of skills}

The hierarchical architectures we propose have a direct impact on the areas covered by random exploration. We will illustrate it with plots showing the visitation of the $(x-y)$ position of the Center of Mass (CoM) during single rollouts of one million steps, each generated with a different architecture. 

On one hand, we show in Fig.\ \ref{fig:visit-trpo-1M} the exploration obtained with actions drawn from a Gaussian with $\mu=0$ and $\Sigma=I$, similar to what would happen in the first iteration of training a Multi-Layer Perceptron with normalized random initialization of the weights. This noise is relatively large as the swimmer robot has actions clipped to $[-1,1]$. Still, it does not yield good exploration as all point reached by the robot during the one million steps rollout lay within the $[-2,2]\times[-2,2]$ box around the initial position. This exploration is not enough to reach the first reward in most of the downstream sparse reward environments and will never learn, as already reported by \citet{duan2016benchmarking}. 

On the other hand, using our hierarchical structures with pretrained policies introduced in Sec.\ \ref{section:method:highlevel} yields a considerable increase in exploration, as reported in Fig.\ \ref{fig:visit-hier-multi-1M}-\ref{fig:visit-hier-snnHB-1M}. These plots also show a possible one millions steps rollout, but under a randomly initialized Manager Network, hence outputting uniformly distributed one-hot vectors every $\mathcal{T}=500$ steps. The color of every point in the CoM trajectory corresponds to the latent code that was fixed at that time-step, clearly showing the ``skill changes". In the following we describe what specific architecture was used for each plot.

The rollout in Fig.\ \ref{fig:visit-hier-multi-1M} is generated following a policy with the \textit{Multi-policy} architecture. This hierarchical architecture is our strong baseline given that it has used six times more samples for pretrain because it trained six policies independently. As explained in Sec.\ \ref{section:method:highlevel}, every $\mathcal{T}=500$ steps it uses the sampled one-hot output of the Manager Network to select one of the six pre-trained policies. We observe that the exploration given by the \textit{Multi-policy} hierarchy heavily concentrates around upward and downward motion, as is expected from the six individual pre-trained policies composing it (refer back to Fig.\ \ref{fig:visit-trpo-individuals}). 

Finally, the rollouts in Fig.\ref{fig:visit-hier-snnBil-1M} and \ref{fig:visit-hier-snnHB-1M} use our hierarchical architecture with a SNN with bilinear integration and $\alpha_H= 0$ and 0.01 respectively. As described in Sec.\ \ref{section:method:highlevel}, the placeholder that during the SNN training received the categorical latent code now receives the one-hot vector output of the Manager Network instead. The exploration obtained with SNNs yields a wider coverage of the space as the underlying policy usually has a larger span of skills. Note how most of the changes of latent code yield a corresponding change in direction. Our simple pre-train setup and the interpretability of the obtained skills are key advantages with respect to previous hierarchical structures. % like seen in \citet{heess2016learning}. %%PA: I am not convinced we should be coming down on the other paper here

\begin{figure}[h!]
	\centering
	\subfigure[Gaussian noise with covariance $\Sigma=I$]{
		\centering
		\label{fig:visit-trpo-1M}
		\includegraphics[trim={0cm 0 0cm 1.2cm}, clip, width = 0.22\textwidth]{Figures/visit-trpo-1M.png}
	}
	\subfigure[Hierarchy with \textit{Multi-policy}]{
		\centering
		\label{fig:visit-hier-multi-1M}
		\includegraphics[trim={2cm 0.7cm 1cm 1.5cm}, clip, width = 0.22\textwidth]{Figures/visit-multi-2.png}
	}
	\subfigure[Hierarchy with Bil-SNN $\alpha_H=0$ ]{
		\centering
		\label{fig:visit-hier-snnBil-1M}
		\includegraphics[trim={2cm 0.7cm 1cm 1.5cm}, clip, width = 0.22\textwidth]{Figures/visit-snn-1Mbisbis.png}
	}
	\subfigure[Hierarchy with Bil-SNN $\alpha_H=0.01$]{
		\centering
		\label{fig:visit-hier-snnHB-1M}
		\includegraphics[trim={2cm 0.7cm 1cm 1.5cm}, clip, width = 0.22\textwidth]{Figures/visit-snnMI-1Mbis.png}
	}
		\vspace{-8pt}
	\caption{Visitation plots for different randomly initialized architectures (one rollout of 1M steps). All axis are in scale [-30,30] and we added a zoom for the Gaussian noise to scale [-2,2]}
	\label{fig:hierarchized-exploration}
\end{figure}

\subsection{Mazes and Gather tasks}
In Fig.\ \ref{fig:learning-curves} we evaluate the learning curves of the different hierarchical architectures proposed. Due to the sparsity of these tasks, none can be properly solved by standard reinforcement algorithms \citep{duan2016benchmarking}. Therefore, we compare our methods against a better baseline: adding to the downstream task the same Center of Mass (CoM) proxy reward that was granted to the robot in the pre-training task. This baseline performs quite poorly in all the mazes, Fig.\ \ref{fig:learn-maze0}-\ref{fig:learn-maze4}. This is due to the long time-horizon needed to reach the goal and the associated credit assignment problem. Furthermore, the proxy reward alone does not encourage diversity of actions as the MI bonus for SNN does. 
The proposed hierarchical architectures are able to learn much faster in every new MDP as they effectively shrink the time-horizon by aggregating time-steps into useful primitives. The benefit of using SNNs with bilinear integration in the hierarchy is also clear over most mazes, although pretraining with MI bonus does not always boost performance. Observing the learned trajectories that solve the mazes, we realize that the turning or sideway motions (more present in SNNs pretrained with MI bonus) help on maze 0, but are not critical in these tasks because the robot may use a specific forward motion against the wall of the maze to reorient itself. We think that the extra skills will shine more in other tasks requiring more precise navigation. And indeed this is the case in the Gather task as seen in Fig.\ \ref{fig:learn-gather}, where not only the average return increases, but also the variance of the learning curve is lower for the algorithm using SNN pretrained with MI bonus, denoting a more consistent learning across different SNNs. For more details on the Gather task, refer to Appendix \ref{sec:gather_compare}.

It is important to mention that each curve of SNN or Multi-policy performance corresponds to a total of 10 runs: 2 random seeds for 5 different SNNs obtained with 5 random seeds in the pretrain task. There is no cherry picking of the pre-trained policy to use in the sparse environment, as done in most previous work. This also explains the high variance of the proposed hierarchical methods in some tasks. This is particularly hard on the mazes as some pretrained SNN may not have the particular motion that allows to reach the goal, so they will have a 0 reward. See Appendix \ref{sec:seed_impact} for more details.

\begin{figure}[h!]
	\centering
	\subfigure[Maze 0]{
		\centering
		\label{fig:learn-maze0}
		\includegraphics[width = 0.4\textwidth]{Figures/learning-Maze0.pdf}
	}
	\subfigure[Maze 1]{
		\centering
		\label{fig:learn-maze1}
		\includegraphics[width = 0.4\textwidth]{Figures/learning-Maze10.pdf}
	}
	\subfigure[Aggregated results for Mazes 2 and 3]{
		\centering
		\label{fig:learn-maze4}
		\includegraphics[width = 0.4\textwidth]{Figures/learning-Maze4.pdf}
	}
	\subfigure[Gather task]{
		\centering
		\label{fig:learn-gather}
		\includegraphics[width = 0.4\textwidth]{Figures/learn-Gather.pdf}
	}
	\caption{Faster learning of the hierarchical architectures in the sparse downstream MDPs}
	\label{fig:learning-curves}
\end{figure}

\section{Discussion and future work}
We propose a framework for first learning a diverse set of skills using stochastic neural networks trained with minimum supervision, and then utilizing these skills in a hierarchical architecture to solve challenging tasks with sparse rewards. Our framework successfully combines two parts, firstly an unsupervised procedure to learn a large span of skills using proxy rewards and secondly a hierarchical structure that encapsulates the latter span of skills and allows to re-use them in future tasks. The span of skills learning can be greatly improved by using stochastic neural networks as policies and their additional expressiveness and multimodality. The bilinear integration and the mutual information bonus are key to consistently yield a wide, interpretable span of skills. As for the hierarchical structure, our experiments demonstrate it can significantly boost the exploration of an agent in a new environment and we demonstrate its relevance for solving complex tasks as mazes or gathering. 

One limitation of our current approach is the switching between skills for unstable agents, as reported in the Appendix \ref{sec:ant-failure} for the “Ant” robot. There are multiple future directions to make our framework more robust to such challenging robots, like learning a transition policy or integrating switching in the pretrain task. 
Other limitations of our current approach are having fixed sub-policies and a fixed switch time $\mathcal{T}$ during the training of downstream tasks. The first issue can be alleviated by introducing end-to-end training, for example using the new straight-through gradient estimators for Stochastic Computations Graphs with discrete latent variables \citep{jang2016gumbel, maddison2016concrete}. The second issue is not critical for static tasks like the ones used here, as studied in Appendix \ref{sec:switch_time}. But in case of becoming a bottleneck for more complex dynamic tasks, a termination policy could be learned by the Manager, similar to the option framework.
Finally, we only used feedforward architectures and hence the decision of what skill to use next only depends on the observation at the moment of switching, not using any sensory information gathered while the previous skill was active. This limitation could be eliminated by introducing a recurrent architecture at the Manager level.

\subsubsection*{Acknowledgments}
This work was supported in part by DARPA, by the Berkeley Vision and Learning Center (BVLC), by the Berkeley Artificial Intelligence Research (BAIR) laboratory, by Berkeley Deep Drive (BDD), and by ONR through a PECASE award. Carlos Florensa was also supported by a La Caixa Ph.D. Fellowship, and Yan Duan by a BAIR Fellowship and a Huawei Fellowship.

\bibliography{ref}
\bibliographystyle{iclr2017_conference}

\appendix
\section{Hyperparameters}
\label{sec:hyper}
All policies are trained with TRPO with step size $0.01$ and discount $0.99$. All neural networks (each of the Multi-policy ones, the SNN and the Manager Network) have 2 layers of 32 hidden units. For the SNN training, the mesh density used to grid the $(x, y)$ space and give the MI bonus is 10 divisions/unit. The number of skills trained (ie dimension of latent variable in the SNN or number of independently trained policies in the Mulit-policy setup) is 6. The batch size and the maximum path length for the pre-train task are also the ones used in the benchmark \citep{duan2016benchmarking}: 50,000 and 500 respectively. For the downstream tasks, see Tab.\ \ref{tab:params}.

\begin{table}[h!]
\centering
\begin{tabular}[t]{l|c|c}
Parameter & Mazes & Food Gather \\
\hline
Batch size & 1M & 100k \\
Maximum path length & 10k & 5k \\
Switch time $\mathcal{T}$ & 500 & 10
\end{tabular}
\caption{Parameters of algorithms for downstream tasks, measured in number of time-steps of the low level control}
\label{tab:params}
\end{table}


\section{Results for Gather with Benchmark settings}
\label{sec:gather_compare}
To fairly compare our methods to previous work on the downstream Gather environment, we report here our results in the exact settings used in \citet{duan2016benchmarking}: maximum path length of 500 and batch-size of 50k. Our SNN hierarchical approach outperforms state-of-the-art intrinsic motivation results like VIME \citep{houthooft2016variational}.

The baseline of having a Center of Mass speed intrinsic reward in the task happens to be stronger than expected. This is due to two factors. First, the task offers a not-so-sparse reward (green and red balls may lie quite close to the robot), which can be easily reached by an agent intrinsically motivated to move. Second, the limit of 500 steps makes the original task much simpler in the sense that the agent only needs to learn how to reach the nearest green ball, as it won't have time to reach anything else. Therefore there is no actual need of skills to re-orient or properly navigate the environment, making the hierarchy useless. This is why when increasing the time-horizon to 5,000 steps, the hierarchy shines much more, as can also be seen in the videos.

\begin{figure}[h!]
	\centering
	\label{fig:learn-gather-benchmark}
	\includegraphics[width = 0.5\textwidth]{Figures/learning-Gather.pdf}
	\caption{Results for Gather environment in the benchmark settings}
\end{figure}


\section{Snake}
\label{sec:snake}

\begin{wrapfigure}{r}{2.5cm}
\vspace{-20pt}
\includegraphics[trim={2cm 3cm 1cm 1.5cm}, clip, width = 2.5cm]{Figures/snake-curl.png}
\vspace{-15pt}
\caption{Snake}
\label{fig:snake-curl}
\end{wrapfigure}

In this section we study how our method scales to a more complex robot. We repeat most experiments from Section 7 with the "Snake" agent, a 5-link robot depicted in Fig.\ \ref{fig:snake-curl}. Compared to Swimmer, the action dimension doubles and the state-space increases by 50\%. We also perform a further analysis on the relevance of the switch time $\mathcal{T}$ and end up by reporting the performance variation among different pretrained SNNs on the hierarchical tasks.

\subsection{Skill learning in pretrain}
\label{sec:snake-pretrain}
The Snake robot can learn a large span of skills as consistently as Swimmer. We report in Figs.\ \ref{fig:visit-snn-snake-10}-\ref{fig:visit-snn-snake-50} the visitation plots obtained for five different random seeds in our SNN pretraining algorithm from Secs.\ \ref{section:method:pretraining}-\ref{section:method:inforeg}, with Mutual Information bonus of 0.05. The impact of the different spans of skills on the later hierarchical training is discussed in Sec.\ \ref{sec:seed_impact}.

\begin{figure}[h!]
	\centering
	\subfigure[seed 10]{
		\centering
		\label{fig:visit-snn-snake-10}
		\includegraphics[trim={2cm 0.7cm 5cm 1.5cm}, clip, width = 0.16\textwidth]{Figures/egoSnake64-snn_005MI_5grid_6lat_categorical_bil_0010_visitation.png}
	}
	\subfigure[seed 20]{
		\centering
		\label{fig:visit-snn-snake-20}
		\includegraphics[trim={2cm 0.7cm 5cm 1.5cm}, clip, width = 0.16\textwidth]{Figures/egoSnake64-snn_005MI_5grid_6lat_categorical_bil_0020_visitation.png}
	}
	\subfigure[seed 30]{
		\centering
		\label{fig:visit-snn-snake-30}
		\includegraphics[trim={2cm 0.7cm 5cm 1.5cm}, clip, width = 0.16\textwidth]{Figures/egoSnake64-snn_005MI_5grid_6lat_categorical_bil_0030_visitation.png}
	}
	\subfigure[seed 40]{
		\centering
		\label{fig:visit-snn-snake-40}
		\includegraphics[trim={2cm 0.7cm 5cm 1.5cm}, clip, width = 0.16\textwidth]{Figures/egoSnake64-snn_005MI_5grid_6lat_categorical_bil_0040_visitation.png}
	}
	\subfigure[seed 50]{
		\centering
		\label{fig:visit-snn-snake-50}
		\includegraphics[trim={2cm 0.7cm 5cm 1.5cm}, clip, width = 0.16\textwidth]{Figures/egoSnake64-snn_005MI_5grid_6lat_categorical_bil_0050_visitation.png}
	}
	\subfigure{
    	\centering
    	\includegraphics[trim={15.5cm 0.7cm 1cm 1.5cm}, clip, width = 0.07\textwidth, height = 0.155\textwidth]{Figures/egoSnake64-snn_005MI_5grid_6lat_categorical_bil_0050_visitation.png}
	}
	\caption{Span of skills learned using different random seeds to pretraining a SNN with MI $= 0.05$}
	\label{fig:visitation-snn-egoSnake}
\end{figure}

\subsection{Hierarchical use of skills in Maze and Gather}
\label{sec:snake-hierarchy}
Here we evaluate the performance of our hierarchical architecture to solve with Snake the sparse tasks Maze 0 and Gather from Sec.\ \ref{sec:experiments}. Given the larger size of the robot, we have also increased the size of the Maze and the Gather task, from 2 to 7 and from 6 to 10 respectively. The maximum path length is also increased in the same proportion, but not the batch size nor the switch time $\mathcal{T}$ (see Sec.\ \ref{sec:switch_time} for an analysis on $\mathcal{T}$). Despite the tasks being harder, our approach still achieves good results, and Figs.\ \ref{fig:egoSnake-maze0}-\ref{fig:egoSnake-gather}, clearly shows how it outperforms the baseline of having the intrinsic motivation of the Center of Mass in the hierarchical task.

\begin{figure}[h!]
	\centering
	\subfigure[Maze 0 with Snake]{
		\centering
		\label{fig:egoSnake-maze0}
		\includegraphics[width = 0.45\textwidth]{Figures/egoSnake-maze0-scaling7-agg500.png}
	}
	\subfigure[Gather task with Snake]{
		\centering
		\label{fig:egoSnake-gather}
		\includegraphics[width = 0.45\textwidth]{Figures/egoSnake-gather-range10-agg10.png}
	}
	\caption{Faster learning of our hierarchical architecture in sparse downstream MDPs with Snake}
	\label{fig:egoSnake-learning-curves}
\end{figure}

\subsection{Analysis of the switch time $\mathcal{T}$}
\label{sec:switch_time}
The switch time $\mathcal{T}$ does not critically affect the performance for these static tasks, where the robot is not required to react fast and precisely to changes in the environment. The performance of very different $\mathcal{T}$ is reported for the Gather task of sizes 10 and 15 in Figs.\ \ref{fig:egoSnake-gather-range10-switch}-\ref{fig:egoSnake-gather-range15-switch}. A smaller environment implies having red/green balls closer to each other, and hence more precise navigation is expected to achieve higher scores. In our framework, lower switch time $\mathcal{T}$ allows faster changes between skills, therefore explaining the slightly better performance of $\mathcal{T}=10$ or 50 over 100 for the Gather task of size 10 (Fig.\ \ref{fig:egoSnake-gather-range10-switch}). On the other hand, larger environments mean more sparcity as the same number of balls is uniformly initialized in a wider space. In such case, longer commitment to every skill is expected to improve the exploration range. This explains the slower learning of $\mathcal{T}=10$ in the Gather task of size 15 (Fig.\ \ref{fig:egoSnake-gather-range15-switch}). It is still surprising the mild changes produced by such large variations in the switch time $\mathcal{T}$ for the Gather task.

For the Maze 0 task, Figs.\ \ref{fig:egoSnake-maze-range7-switch}-\ref{fig:egoSnake-maze-range9-switch} show that the difference between different $\mathcal{T}$ is important but not critical. In particular, radical increases of $\mathcal{T}$ have little effect on the performance in this task because the robot will simply keep bumping against a wall until it switches latent code. This behavior is observed in the videos attached with the paper\textsuperscript{\ref{foot:video}}. The large variances observed are due to the performance of the different pretrained SNN. This is further studied in Sec.\ \ref{sec:seed_impact}.

\begin{figure}[h!]
	\centering
	\subfigure[Gather task with size 10]{
		\centering
		\label{fig:egoSnake-gather-range10-switch}
		\includegraphics[trim={0cm 0cm 0cm 1.5cm}, clip, width = 0.4\textwidth]{Figures/egoSnake-gather-range10-switch.png}
	}
	\subfigure[Gather task with size 15]{
		\centering
		\label{fig:egoSnake-gather-range15-switch}
		\includegraphics[trim={0cm 0cm 0cm 1.5cm}, clip, width = 0.4\textwidth]{Figures/egoSnake-gather-range15-switch.png}
	}
	\subfigure[Maze 0 task with size 7]{
		\centering
		\label{fig:egoSnake-maze-range7-switch}
		\includegraphics[trim={0cm 0cm 0cm 1.5cm}, clip, width = 0.4\textwidth]{Figures/egoSnake-maze-range7-switch.png}
	}
	\subfigure[Maze 0 task with size 9]{
		\centering
		\label{fig:egoSnake-maze-range9-switch}
		\includegraphics[trim={0cm 0cm 0cm 1.5cm}, clip, width = 0.4\textwidth]{Figures/egoSnake-maze-range9-switch.png}
	}
    \vspace{-10pt}    
	\caption{Mild effect of switch time $\mathcal{T}$ on different sizes of Gather and Maze 0}
	\label{fig:egoSnake-learning-curves-switch}
\end{figure}

\subsection{Impact of the pre-trained SNN on the hierarchy}
\label{sec:seed_impact}
In the previous section, the low variance of the learning curves for Gather indicates that all pretrained SNN preform equally good in this task. For Maze, the performance depends strongly on the pretrained SNN. For example we observe that the one obtained with the random seed 50 has a visitation with a weak backward span (Fig.\ \ref{fig:visit-snn-snake-50}), which happens to be critical to solve the Maze 0 (as can be seen in the videos\textsuperscript{\ref{foot:video}}). This explains the lack of learning in Fig.\ \ref{fig:egoSnake-maze0-switch500-pkl} for this particular pretrained SNN. Note that the large variability between different random seeds is even worse in the baseline of having the CoM reward in the Maze, as seen in Fig.\ \ref{fig:egoSnake-maze0-TRPO-seeds}. 3 out of 5 seeds never reach the goal and the ones that does have an unstable learning due to the long time-horizon.

\begin{figure}[h]
	\centering
	\subfigure[Snake on Maze 0 of size 7 and switch-time $\mathcal{T}=500$]{
		\centering
		\label{fig:egoSnake-maze0-switch500-pkl}
		\includegraphics[trim={0cm 0cm 0cm 1.5cm}, clip, width = 0.45\textwidth]{Figures/egoSnake-maze0-range7-agg500-pklSplit.png}
	}
	\subfigure[Snake on Maze 0 of size 7 trained with TRPO and CoM reward]{
		\centering
		\label{fig:egoSnake-maze0-TRPO-seeds}
		\includegraphics[trim={0cm 0cm 0cm 1.5cm}, clip, width = 0.45\textwidth]{Figures/egoSnake-Maze0-scaling9-TRPO-allSeeds.png}
	}	
% 	\subfigure[Maze 0 with Snake and switch time $\mathcal{T}=800$]{
% 		\centering
% 		\label{fig:egoSnake-maze0-switch800-pkl}
% 		\includegraphics[trim={0cm 0cm 0cm 1.5cm}, clip, width = 0.45\textwidth]{Figures/egoSnake-maze0-range7-agg800-pklSplit.png}
% 	}
    \vspace{-10pt}         
	\caption{Variation in performance among different pretrained SNNs and different TRPO seeds}
	\label{fig:egoSnake-learning-curves-pkl}
\end{figure}

\section{Ant}
\label{sec:ant}
\begin{wrapfigure}{r}{2.5cm}
\vspace{-40pt}
\includegraphics[trim={0cm 2cm 1cm 2.5cm}, clip, width = 2.5cm]{Figures/ant.png}
\vspace{-20pt}
\caption{Ant}
\label{fig:ant}
\end{wrapfigure}

In this section we report the progress and failure modes of our approach with more challenging and unstable robots, like Ant (described in \cite{duan2016benchmarking}). This agent has a 27-dimensional space and an 8-dimensional action space. It is unstable in the sense that it might fall over and cannot recover. We show that despite being able to learn well differentiated skills with SNNs, switching between skills is a hard task because of the instability.

\subsection{Sill learning in pretrain}
Our SNN pretraining step is still able to yield large span of skills covering most directions, as long as the MI bonus is well tuned, as observed in Fig.\ \ref{fig:ant-pretrain}. Videos comparing all the different gaits can be observed in the attached videos\textsuperscript{\ref{foot:video}}.

\begin{figure}[h]
    \centering
    \includegraphics[width = 0.8\textwidth]{Figures/ant-pretrain-visitation.png}
    \caption{Pretrain visitation for Ant with different MI bonus}
    \label{fig:ant-pretrain}
\end{figure}

\subsection{Failure modes for unstable robots}
\label{sec:ant-failure}
When switching to a new skill, the robot finds itself in a region of the state-space that might be unknown to the new skill. This increases the instability of the robot, and for example in the case of Ant this might leads to falling over and terminating the rollout. We see this in Fig.\ \ref{fig:ant-failure}, where the 5 depicted rollouts terminate because of the ant falling over. For more details on failure cases with Ant and the possible solutions, please refer to the technical report: \url{https://goo.gl/JjmPqM}


\begin{figure}[h]
    \centering
    \includegraphics[width = 0.4\textwidth]{Figures/ant-failure.png}
    \caption{Failure mode of Ant: here 5 rollouts terminate in less than 6 skill switches.}
    \label{fig:ant-failure}
\end{figure}


% \section{Learning curves of pre-train}

\end{document}
